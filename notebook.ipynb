{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb2af0f",
   "metadata": {},
   "source": [
    "# ç´„ç¶­å®‰è¨ˆç•«ï¼šæ¢¯åº¦éæ¸›\n",
    "\n",
    "> ç¬¬å››åäºŒé€±\n",
    "\n",
    "![giphy.com](https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExZTdwNzRhYmk0dHQ2dG42YnIzemszeGJsYjJwdTRwMXV0dzkwd2MyayZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/QZmKJyPoR2kqChtBqV/giphy-downsized-large.gif)\n",
    "\n",
    "ä¾†æºï¼š<https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExZTdwNzRhYmk0dHQ2dG42YnIzemszeGJsYjJwdTRwMXV0dzkwd2MyayZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/QZmKJyPoR2kqChtBqV/giphy-downsized-large.gif>\n",
    "\n",
    "## ä»€éº¼æ˜¯æ¢¯åº¦éæ¸›\n",
    "\n",
    "åœ¨[ç´„ç¶­å®‰è¨ˆç•«ï¼šæ­£è¦æ–¹ç¨‹å¼](https://datainpoint.substack.com/p/week-forty-one-normal-equations)ä¸€æ–‡ä¸­æˆ‘å€‘æåˆ°äº†æ­£è¦æ–¹ç¨‹å¼ï¼ˆnormal equationsï¼‰ï¼Œé€™æ˜¯é¢å°è¿´æ­¸çš„æ©Ÿå™¨å­¸ç¿’ä»»å‹™æ™‚çš„å‡½æ•¸ç”Ÿæˆæ–¹æ³•ï¼Œä¹Ÿæ˜¯æˆ‘å€‘åœ¨æ©Ÿå™¨å­¸ç¿’ç³»åˆ—æ–‡ç« ä¸­æ‰€å­¸æœƒçš„ç¬¬ä¸€å€‹å­¸ç¿’æ¼”ç®—æ³•ï¼ˆlearning algorithmï¼‰ã€‚åœ¨è³‡æ–™é›†çµ¦å®šç‰¹å¾µçŸ©é™£èˆ‡ç›®æ¨™å‘é‡ï¼Œä»¥åŠé‹ç”¨å‡æ–¹èª¤å·®ï¼ˆmean square errorï¼‰ä½œç‚ºæ•ˆèƒ½è©•ä¼°çš„å‰æä¸‹ï¼Œæˆ‘å€‘èƒ½å¤ è¨ˆç®—æ¬Šé‡å‘é‡ï¼Œé€²è€Œç”Ÿæˆ h å‡½æ•¸ã€‚\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = h(X) = Xw \\; \\text{,where} \\; X \\in \\mathbb{R^{m \\times n}} \\\\\n",
    "w = \\left( X^\\top X \\right)^{-1} X^\\top y \\; \\text{,where} \\; w \\in \\mathbb{R}^n ;\\ y \\in \\mathbb{R}^m\n",
    "\\end{align}\n",
    "\n",
    "ä¸éï¼Œä¸¦éæ‰€æœ‰å‰æç›¸åŒçš„å•é¡Œä»¥æ­£è¦æ–¹ç¨‹å¼æ±‚è§£æ¬Šé‡å‘é‡éƒ½æ˜¯æœ€æœ‰æ•ˆç‡çš„ä½œæ³•ï¼ŒåŸå› æ˜¯åœ¨æ­£è¦æ–¹ç¨‹å¼ä¸­ï¼Œå¿…é ˆè¦æ±‚è§£è½‰ç½®ç‰¹å¾µçŸ©é™£å’Œç‰¹å¾µçŸ©é™£ç›¸ä¹˜å¾Œçš„åçŸ©é™£ï¼Œè½‰ç½®ç‰¹å¾µçŸ©é™£çš„å¤–å‹ç‚º (n, m)ï¼Œè€Œç‰¹å¾µçŸ©é™£çš„å¤–å‹ç‚º (m, n)ï¼Œå…©è€…ç›¸ä¹˜ä¹‹å¾Œçš„çŸ©é™£å¤–å‹ç‚º (n, n)ã€‚\n",
    "\n",
    "\\begin{align}\n",
    "X^\\top \\in \\mathbb{R^{n \\times m}} \\\\\n",
    "X \\in \\mathbb{R^{m \\times n }} \\\\\n",
    "\\left( X^\\top X \\right)^{-1} \\in \\mathbb{R^{n \\times n}}\n",
    "\\end{align}\n",
    "\n",
    "æ±‚è§£ä¸€å€‹å¤–å‹ç‚º (n, n) çš„åçŸ©é™£ï¼Œæ¨™æº–çš„æ™‚é–“è¨ˆç®—è¤‡é›œåº¦æ˜¯å¤§ O ç¬¦è™Ÿ n çš„ç«‹æ–¹ $O(n^3)$ï¼Œå› æ­¤ç•¶ç‰¹å¾µçŸ©é™£çš„è®Šæ•¸å¢åŠ  10 å€æ™‚ï¼Œæ‰€éœ€è¦çš„è¨ˆç®—æ™‚é–“ä¸¦ä¸æ˜¯å¢åŠ  10 å€ï¼Œè€Œæ˜¯å¢åŠ  1000 å€ï¼Œç‚ºæ­¤åœ¨å› æ‡‰ n å¾ˆå¤§çš„ç‹€æ³æ™‚ï¼Œæˆ‘å€‘æœƒæ”¹ç”¨ç¬¬äºŒå€‹å­¸ç¿’æ¼”ç®—æ³•ï¼šæ¢¯åº¦éæ¸›ï¼ˆgradient descentï¼‰ã€‚å…¶ä¸­æ¢¯åº¦ï¼ˆgradientï¼‰æŒ‡çš„æ˜¯åœ¨å‘é‡å°å‡½æ•¸çš„åå¾®åˆ†ï¼Œå°æ‡‰å°æ•¸ï¼ˆderivativeï¼‰æŒ‡çš„æ˜¯ç´”é‡å°å‡½æ•¸çš„åå¾®åˆ†ã€‚æ¢¯åº¦éæ¸›æŒ‡çš„æ˜¯ï¼Œåœ¨è³‡æ–™é›†çµ¦å®šç‰¹å¾µçŸ©é™£èˆ‡ç›®æ¨™å‘é‡ï¼Œä¸”æ•ˆèƒ½è©•ä¼°çš„å‡½æ•¸å¯ä»¥åšæœ€å°åŒ–æˆ–æœ€å¤§åŒ–çš„å‰æä¸‹ï¼Œæˆ‘å€‘èƒ½å¤ ä»¥å¤šæ¬¡è¿­ä»£çš„æ–¹å¼å„ªåŒ–æ¬Šé‡å‘é‡ï¼Œé€²è€Œä»¥å„ªåŒ–éå¾Œçš„æ¬Šé‡å‘é‡ç”Ÿæˆ h å‡½æ•¸ã€‚å…¶ä¸­ w æ˜¯éš¨æ©Ÿç”Ÿæˆçš„æ¬Šé‡å‘é‡ï¼ŒJ å‡½æ•¸æ˜¯æˆæœ¬å‡½æ•¸ï¼ˆcost functionï¼‰ï¼Œä¹Ÿå°±æ˜¯ç”¨ä¾†é€²è¡Œæ•ˆèƒ½è©•ä¼°çš„å‡½æ•¸ï¼Œæœ‰æ™‚ä¹Ÿå¯«åš L å‡½æ•¸å³æå¤±å‡½æ•¸ï¼ˆloss functionï¼‰ï¼Œè€Œ epsilon å‰‡æ˜¯ä¸€å€‹æ­£çš„ç´”é‡ï¼Œç¨±ä½œå­¸ç¿’é€Ÿç‡ï¼ˆlearning rateï¼‰ï¼Œæ˜¯ä¸€å€‹æ±ºå®šå„ªåŒ–å¹…åº¦çš„æ­£æ•¸ç´”é‡ã€‚\n",
    "\n",
    "\\begin{flalign}\n",
    "&\\text{For each epoch:} \\\\\n",
    "&w := w - \\epsilon \\nabla_w J(w) \\; \\text{, where } \\epsilon \\in \\mathbb{R}^+ \n",
    "\\end{flalign}\n",
    "\n",
    "![](001.png)\n",
    "\n",
    "ä¸Šåœ–ä¸­çš„è—è‰²è™›ç·šæç¹ªä¸€å€‹æˆæœ¬å‡½æ•¸ fï¼Œç¶ è‰²å¯¦ç·šæç¹ªè©²å‡½æ•¸çš„å°æ•¸ï¼Œå°æ–¼ x < 0 çš„éƒ¨åˆ†å°æ•¸ç‚ºè² ï¼Œæ‰€ä»¥ x æœƒå‘å³ç§»å‹•ï¼›å°æ–¼ x > 0 çš„éƒ¨åˆ†å°æ•¸ç‚ºæ­£ï¼Œæ‰€ä»¥ x æœƒå‘å·¦ç§»å‹•ï¼›å°æ–¼ x = 0 çš„éƒ¨åˆ†å°æ•¸ç‚ºé›¶ï¼Œx ä¸ç§»å‹•ã€‚\n",
    "\n",
    "## æ•ˆèƒ½è©•ä¼°\n",
    "\n",
    "ä»¥å‡æ–¹èª¤å·®ï¼ˆmean square errorï¼‰ä½œç‚ºæˆæœ¬å‡½æ•¸ J ä¾†ç•¶ä¾‹å­ï¼Œæˆ‘å€‘å¯ä»¥æ¨å°æ¢¯åº¦çš„è¡¨ç¤ºå¼å¦‚ä¸‹ã€‚\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Let } J(w) &= MSE \\\\\n",
    "&= \\frac{1}{m}  || \\hat{y} - y ||^2 \\\\\n",
    "&= \\frac{1}{m} || h(X) - y ||^2\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_wJ(w) &= \\nabla_w \\left( \\frac{1}{m} || h(X) - y ||^2 \\right)  \\\\\n",
    "&= \\frac{1}{m} \\nabla_w|| Xw - y ||^2 \\\\\n",
    "&= \\frac{1}{m} \\nabla_w \\left( Xw - y \\right)^\\top\\left( Xw - y \\right) \\\\\n",
    "&= \\frac{1}{m} \\nabla_w \\left( w^\\top X^\\top Xw - 2w^\\top X^\\top y + y^\\top y \\right) \\\\\n",
    "&= \\frac{1}{m} \\left( 2X^\\top X w - 2X^\\top y \\right) \\\\\n",
    "&= \\frac{2}{m} X^\\top \\left(Xw - y\\right) \\\\\n",
    "&= \\frac{2}{m} X^\\top \\left(\\hat{y} - y\\right)\n",
    "\\end{align}\n",
    "\n",
    "å› æ­¤ï¼Œç•¶ä»¥å‡æ–¹èª¤å·®ä½œç‚ºæˆæœ¬å‡½æ•¸æ™‚ï¼Œæˆ‘å€‘å¯ä»¥å°‡æ¢¯åº¦éæ¸›çš„æ³›å‹å¼è¡¨ç¤ºå¦‚ä¸‹ã€‚\n",
    "\n",
    "\\begin{align}\n",
    "&\\text{For each epoch:} \\\\\n",
    "&w := w - \\epsilon \\frac{2}{m} X^\\top \\left(\\hat{y} - y\\right) \\; \\text{, where } \\epsilon \\in \\mathbb{R}^+\n",
    "\\end{align}\n",
    "\n",
    "## ä»¥ Numpy å¯¦ä½œæ¢¯åº¦éæ¸›\n",
    "\n",
    "æˆåŠŸæ¨å°å‡ºæ­£è¦æ–¹ç¨‹å¼ä¹‹å¾Œï¼Œæœ€å¾Œæˆ‘å€‘å¯«ä½œä¸€å€‹é¡åˆ¥ `GradientDescent`ï¼Œä»¥ Python èˆ‡ Numpy æ¨¡çµ„å¯¦ä½œï¼Œåœ¨çµ¦å®šé©ç•¶è³‡æ–™é›†çš„ç‰¹å¾µçŸ©é™£èˆ‡ç›®æ¨™å‘é‡ï¼Œèƒ½å¤ è¿­ä»£å„ªåŒ–éš¨æ©Ÿç”Ÿæˆçš„æ¬Šé‡å‘é‡ï¼Œé€²è€Œç”Ÿæˆ h å‡½æ•¸ã€‚å¯¦ä½œéç¨‹ä¸­æˆ‘å€‘æœƒç”¨åˆ° Numpy æ¨¡çµ„çš„ `np.transpose()` æˆ– ndarray çš„ `T` å±¬æ€§ä¾†ç”Ÿæˆè½‰ç½®çŸ©é™£ã€`np.dot()` æˆ– ndarray çš„ `dot()` æ–¹æ³•ä¾†è¨ˆç®—å‘é‡èˆ‡çŸ©é™£çš„ç›¸ä¹˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786ef1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14883492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self._fit_intercept = fit_intercept\n",
    "    def find_gradient(self):\n",
    "        y_hat = np.dot(self._X_train, self._w)\n",
    "        gradient = (2/self._m) * np.dot(self._X_train.T, y_hat - self._y_train)\n",
    "        return gradient\n",
    "    def mean_squared_error(self):\n",
    "        y_hat = np.dot(self._X_train, self._w)\n",
    "        mse = ((y_hat - self._y_train).T.dot(y_hat - self._y_train)) / self._m\n",
    "        return mse\n",
    "    def fit(self, X_train, y_train, epochs=10000, learning_rate=0.001):\n",
    "        self._X_train = X_train.copy()\n",
    "        self._y_train = y_train.copy()\n",
    "        self._m = self._X_train.shape[0]\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((self._m, 1), dtype=float)\n",
    "            self._X_train = np.concatenate([X0, self._X_train], axis=1)\n",
    "        n = self._X_train.shape[1]\n",
    "        self._w = np.random.rand(n)\n",
    "        n_prints = 10\n",
    "        print_iter = epochs // n_prints\n",
    "        w_history = dict()\n",
    "        for i in range(epochs):\n",
    "            current_w = self._w.copy()\n",
    "            w_history[i] = current_w\n",
    "            mse = self.mean_squared_error()\n",
    "            gradient = self.find_gradient()\n",
    "            if i % print_iter == 0:\n",
    "                print(\"epoch: {:6} - loss: {:.6f}\".format(i, mse))\n",
    "            self._w -= learning_rate*gradient\n",
    "        w_ravel = self._w.copy().ravel()\n",
    "        self.intercept_ = w_ravel[0]\n",
    "        self.coef_ = w_ravel[1:]\n",
    "        self._w_history = w_history\n",
    "    def predict(self, X_test):\n",
    "        self._X_test = X_test\n",
    "        m = self._X_test.shape[0]\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((m, 1), dtype=float)\n",
    "            self._X_test = np.concatenate([X0, self._X_test], axis=1)\n",
    "        y_pred = np.dot(self._X_test, self._w)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b68ba55",
   "metadata": {},
   "source": [
    "`GradientDescent` é¡åˆ¥çš„ä½¿ç”¨æ˜¯ä»¥ `fit()` æ–¹æ³•ç”Ÿæˆæ¬Šé‡å‘é‡ï¼Œä¸¦ä»¥ `predict()` æ–¹æ³•ç”Ÿæˆé æ¸¬ã€‚æˆ‘å€‘å¯ä»¥å°‡ `GradientDescent` é¡åˆ¥æ‡‰ç”¨åœ¨ Kaggle çš„ House Prices è³‡æ–™é›†ï¼Œè—‰æ­¤ä¾†é©—è­‰å®ƒçš„å¯ç”¨æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375e469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:      0 - loss: 39038348092.790665\n",
      "epoch:   1000 - loss: 2771553803.375100\n",
      "epoch:   2000 - loss: 2700216616.814837\n",
      "epoch:   3000 - loss: 2641272887.047218\n",
      "epoch:   4000 - loss: 2592569490.593770\n",
      "epoch:   5000 - loss: 2552327367.547720\n",
      "epoch:   6000 - loss: 2519076535.261282\n",
      "epoch:   7000 - loss: 2491602392.148215\n",
      "epoch:   8000 - loss: 2468901350.162414\n",
      "epoch:   9000 - loss: 2450144175.275456\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"https://raw.githubusercontent.com/datainpoint/week-forty-two-gradient-descent/main/train.csv\")\n",
    "test = pd.read_csv(\"https://raw.githubusercontent.com/datainpoint/week-forty-two-gradient-descent/main/test.csv\")\n",
    "X_train = train[\"OverallQual\"].values.reshape(-1, 1)\n",
    "y_train = train[\"SalePrice\"].values\n",
    "X_test = test[\"OverallQual\"].values.reshape(-1, 1)\n",
    "gradient_descent = GradientDescent()\n",
    "gradient_descent.fit(X_train, y_train)\n",
    "y_pred = gradient_descent.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932dbf80",
   "metadata": {},
   "source": [
    "ç¬¬å››åäºŒé€±ç´„ç¶­å®‰è¨ˆç•«ï¼šæ¢¯åº¦éæ¸›ä¾†åˆ°å°¾è²ï¼Œå¸Œæœ›æ‚¨ä¹Ÿå’Œæˆ‘ä¸€æ¨£æœŸå¾…ä¸‹ä¸€ç¯‡æ–‡ç« ã€‚å°æ–¼é€™ç¯‡æ–‡ç« æœ‰ä»€éº¼æƒ³æ³•å‘¢ï¼Ÿå–œæ­¡ğŸ˜»ã€åˆ†äº«ğŸ™Œã€è¨‚é–±ğŸ“¨æˆ–è€…ç•™è¨€ğŸ™‹â€â™‚ï¸"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
